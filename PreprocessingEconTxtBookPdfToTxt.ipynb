{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNUzlB0U9vKq2gxXVtSDyXY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/summerolmstead/EconomicsLLM/blob/main/PreprocessingEconTxtBookPdfToTxt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QE52-wujgpKh",
        "outputId": "114fc481-c865-4bd2-c388-1acd0b6e1e86"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfplumber"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMQp7J4AhdmZ",
        "outputId": "9102c924-690b-4a88-8c07-8132b6b7b71d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.4-py3-none-any.whl.metadata (41 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six==20231228 (from pdfplumber)\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (11.0.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.4.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
            "Downloading pdfplumber-0.11.4-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20231228 pdfplumber-0.11.4 pypdfium2-4.30.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber\n",
        "import os\n",
        "\n",
        "pdf_path = \"/content/drive/MyDrive/EconLLM/Macroeconomics2.pdf\"\n",
        "\n",
        "with pdfplumber.open(pdf_path) as pdf:\n",
        "    text = \"\"\n",
        "    for page in pdf.pages:\n",
        "        text += page.extract_text()  #extract text from each page\n",
        "\n",
        "folder_path = os.path.dirname(pdf_path)\n",
        "\n",
        "output_text_path = os.path.join(folder_path, \"extracted_text.txt\")\n",
        "\n",
        "#save the extracted text to the output text file\n",
        "with open(output_text_path, \"w\") as text_file:\n",
        "    text_file.write(text)\n",
        "\n",
        "#print success message with the file path\n",
        "print(f\"Text file saved to: {output_text_path}\")\n",
        "\n",
        "#show a snippet of the extracted text to see what it looks like\n",
        "print(text[:1000])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2CFcA-7jfsq",
        "outputId": "881fcad9-1411-412d-d80c-4c01f86953cd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text file saved to: /content/drive/MyDrive/EconLLM/extracted_text.txt\n",
            "Principles of\n",
            "Macroeconomics 2e\n",
            "SENIOR CONTRIBUTING AUTHORS\n",
            "STEVEN A. GREENLAW, UNIVERSITY OF MARY WASHINGTON\n",
            "DAVID SHAPIRO, PENNSYLVANIA STATE UNIVERSITY\n",
            "Based on the 2nd edition of Principles of\n",
            "Economics, Economics and the Economy, 2e\n",
            "by Timothy Taylor, published in 2011.OpenStax\n",
            "Rice University\n",
            "6100 Main Street MS-375\n",
            "Houston, Texas 77005\n",
            "To learn more about OpenStax, visit https://openstax.org.\n",
            "Individual print copies and bulk orders can be purchased through our website.\n",
            "©2018 Rice University. Textbook content produced by OpenStax is licensed under a Creative Commons\n",
            "Attribution 4.0 International License (CC BY 4.0). Under this license, any user of this textbook or the textbook\n",
            "contents herein must provide proper attribution as follows:\n",
            "- If you redistribute this textbook in a digital format (including but not limited to PDF and HTML), then you\n",
            "must retain on every page the following attribution:\n",
            "“Download for free at https://openstax.org/details/books/principles-macroeconomics-2e\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ok so now i am going to move the extracted text i deleted the intro and need ome reg ex to clean the data and take out some parts.... ome sentenances are merged into a word so without\n",
        "#spaces as well so i will try to fix all of those as well!\n"
      ],
      "metadata": {
        "id": "o29Yozj5nEGP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "#function to clean up merged words, numbers, and other formatting issues\n",
        "def remove_unwanted_content2(text):\n",
        "\n",
        "    #remove table references (e.g., \"Table 4.6\")\n",
        "    text = re.sub(r'Table \\d+\\.\\d+', '', text)\n",
        "\n",
        "    #remove this specific sentence that shows up often\n",
        "    text = re.sub(r'This OpenStax book is available for free at http[s]?://\\S+', '', text)\n",
        "\n",
        "    #remove URLs (e.g., \"http://cnx.org/content/col12190/1.4\") greedy regex\n",
        "    text = re.sub(r'http[s]?://\\S+', '', text)\n",
        "\n",
        "    #clean up any excessive whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
        "\n",
        "    #trim leading and trailing spaces\n",
        "    text = text.strip()\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "Xd8OPO5UZd09"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wordninja"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bt4GGs9YslYB",
        "outputId": "ece87874-9622-4ac5-e9ac-e3635b2e5dc6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wordninja\n",
            "  Downloading wordninja-2.0.0.tar.gz (541 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/541.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.0/541.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m541.6/541.6 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wordninja\n",
            "  Building wheel for wordninja (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wordninja: filename=wordninja-2.0.0-py3-none-any.whl size=541530 sha256=bb46cc3d0d3abbddd6dd51eea7cc7816b08af39c3c9169db359aae4bbeac89de\n",
            "  Stored in directory: /root/.cache/pip/wheels/aa/44/3a/f2a5c1859b8b541ded969b4cd12d0a58897f12408f4f51e084\n",
            "Successfully built wordninja\n",
            "Installing collected packages: wordninja\n",
            "Successfully installed wordninja-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wordninja\n",
        "\n",
        "#function to fix merged words using wordninja\n",
        "def fix_merged_words_with_ninja(text):\n",
        "    cleaned_text = ' '.join(wordninja.split(text))  #split the words and join them with a space\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "#cxample of merged word text\n",
        "merged_text = \"Economicsisconcernedwiththewell-beingofallpeoplewhohavenotjobs\"\n",
        "\n",
        "#apply the fix_merged_words_with_ninja function to the merged text\n",
        "cleaned_text = fix_merged_words_with_ninja(merged_text)\n",
        "\n",
        "#print the cleaned text\n",
        "print(cleaned_text)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Brv02CS6qzjy",
        "outputId": "6c01ff31-cd50-4a58-8d7b-b938f8162c85"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Economics is concerned with the well being of all people who have not jobs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#path for editing output of manually deleted intro and end\n",
        "txtfileedit1_path = \"/content/drive/MyDrive/EconLLM/MacroeconomicsTxtBookEdit1.txt\"\n",
        "\n",
        "#step 1: Read the text from the file !!!!\n",
        "with open(txtfileedit1_path, \"r\") as file:\n",
        "    text = file.read()\n",
        "\n",
        "#step 2: Apply the cleaning functions we just defined keeping chapter and subsection for easier training process\n",
        "cleaned_text = remove_unwanted_content2(text)\n",
        "cleaned_text = fix_merged_words_with_ninja(cleaned_text)\n",
        "\n",
        "#step 3: Save the cleaned text back to a file in the same folder\n",
        "output_path = \"/content/drive/MyDrive/EconLLM/MacroeconomicsTxtBookEdit4.txt\"\n",
        "\n",
        "with open(output_path, \"w\") as file:\n",
        "    file.write(cleaned_text)\n",
        "\n",
        "#success message\n",
        "print(f\"Cleaned text saved to: {output_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naZ505YgppXA",
        "outputId": "20d4ded7-6835-4d97-aa06-3c43f14b1d52"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned text saved to: /content/drive/MyDrive/EconLLM/MacroeconomicsTxtBookEdit4.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decided To train sections probably should keep chapters and subsections for the easiest way to automate all data for model 12/27"
      ],
      "metadata": {
        "id": "7VkE6VGkY0hM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import os\n",
        "\n",
        "def parse_text_to_chapters_and_subsections(text):\n",
        "    # regex pattern to match chapter headings (e.g., \"chapter 1 introduction\")\n",
        "    chapter_pattern = r\"(Chapter \\d+ [^\\n]+)\"\n",
        "\n",
        "    # regex pattern to match subsections like \"1 5\", \"2 1\", etc.\n",
        "    subsection_pattern = r\"(\\d+ \\d+ [^\\n]+)\"\n",
        "\n",
        "    # split the text into potential chapters based on the chapter heading pattern\n",
        "    split_chapters = re.split(f\"({chapter_pattern})\", text)\n",
        "\n",
        "    chapter_data = {}\n",
        "    current_chapter = None\n",
        "    current_subsection = None\n",
        "\n",
        "    print(f\"total parts split: {len(split_chapters)}\")  # debugging: check how many parts were split\n",
        "\n",
        "    for part in split_chapters:\n",
        "        part = part.strip()  # clean up the part\n",
        "\n",
        "        # debugging: check each part being processed\n",
        "        print(f\"processing: {part[:30]}...\")  # show the first 30 characters for context\n",
        "\n",
        "        # check if this part matches a chapter heading\n",
        "        if re.match(chapter_pattern, part):\n",
        "            current_chapter = part\n",
        "            chapter_data[current_chapter] = []  # initialize a list for the chapter content\n",
        "            current_subsection = None  # reset subsection when a new chapter starts\n",
        "        # check if this part matches a subsection heading\n",
        "        elif re.match(subsection_pattern, part):\n",
        "            current_subsection = part\n",
        "            # append the subsection under the current chapter as a dictionary\n",
        "            chapter_data[current_chapter].append({current_subsection: []})\n",
        "        else:\n",
        "            # if the part is content, append it under the current subsection (if active)\n",
        "            if current_subsection:\n",
        "                # append content to the current subsection\n",
        "                chapter_data[current_chapter][-1][current_subsection].append(part)\n",
        "            elif current_chapter:\n",
        "                # if no subsection is active, append content directly to the chapter\n",
        "                chapter_data[current_chapter].append(part)\n",
        "\n",
        "    print(f\"parsed chapters: {len(chapter_data)}\")  # debugging: check how many chapters were parsed\n",
        "    return chapter_data\n",
        "\n",
        "# read the text from the input file\n",
        "def read_input_file(input_path):\n",
        "    if not os.path.exists(input_path):\n",
        "        print(f\"error: file {input_path} not found!\")\n",
        "        return \"\"\n",
        "\n",
        "    with open(input_path, \"r\") as file:\n",
        "        text = file.read()\n",
        "\n",
        "    print(f\"read {len(text)} characters from input file.\")  # debugging: check how much text was read\n",
        "    return text\n",
        "\n",
        "# function to write the parsed data to a text file\n",
        "def write_parsed_data_to_text(parsed_data, output_path):\n",
        "    with open(output_path, \"w\") as file:\n",
        "        if not parsed_data:\n",
        "            print(\"warning: no data to write.\")  # debugging: check if data is empty\n",
        "        for chapter, subsections in parsed_data.items():\n",
        "            file.write(f\"{chapter}\\n\")\n",
        "            for subsection_data in subsections:\n",
        "                # handle subsections\n",
        "                if isinstance(subsection_data, dict):\n",
        "                    for subsection, content in subsection_data.items():\n",
        "                        file.write(f\"  {subsection}\\n\")\n",
        "                        for item in content:\n",
        "                            file.write(f\"    {item}\\n\")\n",
        "                else:\n",
        "                    # if it's not a dictionary (i.e., it's content directly under the chapter)\n",
        "                    file.write(f\"  {subsection_data}\\n\")\n",
        "            file.write(\"\\n\")  # newline to separate chapters\n",
        "\n",
        "# path to input and output files\n",
        "input_path = \"/content/drive/MyDrive/EconLLM/MacroeconomicsTxtBookEdit4.txt\"\n",
        "output_path = \"/content/drive/MyDrive/EconLLM/MacroeconomicsProcessSectionsResult3.txt\"\n",
        "\n",
        "# read the input text\n",
        "text = read_input_file(input_path)\n",
        "\n",
        "# if the text is valid, parse it into chapters and subsections\n",
        "if text:\n",
        "    parsed_data = parse_text_to_chapters_and_subsections(text)\n",
        "    # write the parsed data to the output file\n",
        "    write_parsed_data_to_text(parsed_data, output_path)\n",
        "\n",
        "    # success message\n",
        "    print(f\"parsed data saved to: {output_path}\")\n",
        "else:\n",
        "    print(\"no content read from the file. aborting further processing.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WH7VE_0-dc9s",
        "outputId": "c763e67b-5412-4490-8449-6002da57bf35"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "read 1644234 characters from input file.\n",
            "total parts split: 7\n",
            "processing: Senior contributing authors St...\n",
            "processing: Chapter 1 Welcome to Economics...\n",
            "processing: Chapter 1 Welcome to Economics...\n",
            "processing: ...\n",
            "processing: Chapter 1 Welcome to Economics...\n",
            "processing: Chapter 1 Welcome to Economics...\n",
            "processing: ...\n",
            "parsed chapters: 2\n",
            "parsed data saved to: /content/drive/MyDrive/EconLLM/MacroeconomicsProcessSectionsResult3.txt\n"
          ]
        }
      ]
    }
  ]
}
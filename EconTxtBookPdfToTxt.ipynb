{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMteF/c/eVbf8PIhMQi5Nhr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/summerolmstead/EconomicsLLM/blob/main/EconTxtBookPdfToTxt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QE52-wujgpKh",
        "outputId": "630d1859-ddd7-426f-9f58-f5ebcfbaf1de"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfplumber"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMQp7J4AhdmZ",
        "outputId": "807fc4ec-ef3b-42a1-c994-40e46adc4641"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.4-py3-none-any.whl.metadata (41 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six==20231228 (from pdfplumber)\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (11.0.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.4.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
            "Downloading pdfplumber-0.11.4-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20231228 pdfplumber-0.11.4 pypdfium2-4.30.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber\n",
        "import os\n",
        "\n",
        "pdf_path = \"/content/drive/MyDrive/EconLLM/Macroeconomics2.pdf\"\n",
        "\n",
        "with pdfplumber.open(pdf_path) as pdf:\n",
        "    text = \"\"\n",
        "    for page in pdf.pages:\n",
        "        text += page.extract_text()  #extract text from each page\n",
        "\n",
        "folder_path = os.path.dirname(pdf_path)\n",
        "\n",
        "output_text_path = os.path.join(folder_path, \"extracted_text.txt\")\n",
        "\n",
        "#save the extracted text to the output text file\n",
        "with open(output_text_path, \"w\") as text_file:\n",
        "    text_file.write(text)\n",
        "\n",
        "#print success message with the file path\n",
        "print(f\"Text file saved to: {output_text_path}\")\n",
        "\n",
        "#show a snippet of the extracted text to see what it looks like\n",
        "print(text[:1000])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2CFcA-7jfsq",
        "outputId": "35bb924e-e8a0-4a6d-c45c-cf2a159069ad"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text file saved to: /content/drive/MyDrive/EconLLM/extracted_text.txt\n",
            "Principles of\n",
            "Macroeconomics 2e\n",
            "SENIOR CONTRIBUTING AUTHORS\n",
            "STEVEN A. GREENLAW, UNIVERSITY OF MARY WASHINGTON\n",
            "DAVID SHAPIRO, PENNSYLVANIA STATE UNIVERSITY\n",
            "Based on the 2nd edition of Principles of\n",
            "Economics, Economics and the Economy, 2e\n",
            "by Timothy Taylor, published in 2011.OpenStax\n",
            "Rice University\n",
            "6100 Main Street MS-375\n",
            "Houston, Texas 77005\n",
            "To learn more about OpenStax, visit https://openstax.org.\n",
            "Individual print copies and bulk orders can be purchased through our website.\n",
            "©2018 Rice University. Textbook content produced by OpenStax is licensed under a Creative Commons\n",
            "Attribution 4.0 International License (CC BY 4.0). Under this license, any user of this textbook or the textbook\n",
            "contents herein must provide proper attribution as follows:\n",
            "- If you redistribute this textbook in a digital format (including but not limited to PDF and HTML), then you\n",
            "must retain on every page the following attribution:\n",
            "“Download for free at https://openstax.org/details/books/principles-macroeconomics-2e\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ok so now i am going to move the extracted text i deleted the intro and need ome reg ex to clean the data and take out some parts.... ome sentenances are merged into a word so without\n",
        "#spaces as well so i will try to fix all of those as well!\n"
      ],
      "metadata": {
        "id": "o29Yozj5nEGP"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "#function to clean up merged words, numbers, and other formatting issues\n",
        "def remove_unwanted_content(text):\n",
        "\n",
        "    #remove table references (e.g., \"Table 4.6\")\n",
        "    text = re.sub(r'Table \\d+\\.\\d+', '', text)\n",
        "\n",
        "    #remove this specific sentence that shows up often\n",
        "    text = re.sub(r'This OpenStax book is available for free at http[s]?://\\S+', '', text)\n",
        "\n",
        "    #remove URLs (e.g., \"http://cnx.org/content/col12190/1.4\")\n",
        "    text = re.sub(r'http[s]?://\\S+', '', text)\n",
        "\n",
        "    #remove chapter references (e.g., \"Chapter 5\")\n",
        "    text = re.sub(r'Chapter \\d+', '', text)\n",
        "\n",
        "    #remove section headers (e.g., \"5 | Elasticity\")\n",
        "    text = re.sub(r'\\d+ \\| [A-Za-z\\s]+', '', text)\n",
        "\n",
        "    #clean up any excessive whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
        "\n",
        "    #trim leading and trailing spaces\n",
        "    text = text.strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "#example test\n",
        "problematic_text = \"\"\"\n",
        "10% 155 110\n",
        "Table 4.6\n",
        "This OpenStax book is available for free at http://cnx.org/content/col12190/1.4Chapter 5 | Elasticity 107\n",
        "5 | Elasticity\n",
        "\"\"\"\n",
        "\n",
        "cleaned_text = remove_unwanted_content(problematic_text)\n",
        "\n",
        "#test result\n",
        "print(cleaned_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-psemeJnT99",
        "outputId": "475038f2-9103-4467-e2f2-272f30492332"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10% 155 110 107\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wordninja"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bt4GGs9YslYB",
        "outputId": "82f02d2d-6ccd-43cf-a83b-26f90178f295"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wordninja\n",
            "  Downloading wordninja-2.0.0.tar.gz (541 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/541.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.0/541.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m541.6/541.6 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wordninja\n",
            "  Building wheel for wordninja (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wordninja: filename=wordninja-2.0.0-py3-none-any.whl size=541530 sha256=2087929297e0841bbe6c48e4575fa8572afbb1e6405420bda5dd6c4bc675602e\n",
            "  Stored in directory: /root/.cache/pip/wheels/aa/44/3a/f2a5c1859b8b541ded969b4cd12d0a58897f12408f4f51e084\n",
            "Successfully built wordninja\n",
            "Installing collected packages: wordninja\n",
            "Successfully installed wordninja-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wordninja\n",
        "\n",
        "#function to fix merged words using wordninja\n",
        "def fix_merged_words_with_ninja(text):\n",
        "    cleaned_text = ' '.join(wordninja.split(text))  #split the words and join them with a space\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "#cxample of merged word text\n",
        "merged_text = \"Economicsisconcernedwiththewell-beingofallpeoplewhohavenotjobs\"\n",
        "\n",
        "#apply the fix_merged_words_with_ninja function to the merged text\n",
        "cleaned_text = fix_merged_words_with_ninja(merged_text)\n",
        "\n",
        "#print the cleaned text\n",
        "print(cleaned_text)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Brv02CS6qzjy",
        "outputId": "78ebb5d6-6b7b-4871-b6db-6a1b926b48b2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Economics is concerned with the well being of all people who have not jobs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#path for editing output of manually deleted intro and end\n",
        "txtfileedit1_path = \"/content/drive/MyDrive/EconLLM/MacroeconomicsTxtBookEdit1.txt\"\n",
        "\n",
        "#step 1: Read the text from the file !!!!\n",
        "with open(txtfileedit1_path, \"r\") as file:\n",
        "    text = file.read()\n",
        "\n",
        "#step 2: Apply the cleaning functions we just defined\n",
        "cleaned_text = remove_unwanted_content(text)\n",
        "cleaned_text = fix_merged_words_with_ninja(cleaned_text)\n",
        "\n",
        "#step 3: Save the cleaned text back to a file in the same folder\n",
        "output_path = \"/content/drive/MyDrive/EconLLM/MacroeconomicsTxtBookEdit3.txt\"\n",
        "\n",
        "with open(output_path, \"w\") as file:\n",
        "    file.write(cleaned_text)\n",
        "\n",
        "#success message\n",
        "print(f\"Cleaned text saved to: {output_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naZ505YgppXA",
        "outputId": "6e87d621-3e4e-4695-8312-c78558289d31"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned text saved to: /content/drive/MyDrive/EconLLM/MacroeconomicsTxtBookEdit3.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "for now, this is fine before we make the question and answer pair when we deep dive into creating our model!"
      ],
      "metadata": {
        "id": "wHxjLmYUwXs3"
      }
    }
  ]
}